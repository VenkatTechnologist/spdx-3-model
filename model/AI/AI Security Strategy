1. # AI Security Strategy

## Classifications

The **AI Security Strategy** can be broadly classified into two, based on SPDX SBOM perspective.

1. Another software party using the AI system for their work (including vulnerability management)
2. End-user customer using the AI system (including vulnerability management)

Things to consider include:

### Third-party software using the AI system

1. Data Privacy
2. Model security
3. Access Control
4. API security
5. Secure communications
6. Threat detection & monitoring
7. Governance, Regulation, and Compliance (GRC) & auditing
8. Supply Chain security
9. Data Ownership & sharing details
10. Model explainability
11. Model performance
12. Model maintainability
13. Vulnerabilities (go into one of the one of the previous sections: data poisoning, model poisoning, neural trojan attacks, model transfer attacks, malicious
    model files and dependencies, dependency confusion, typo-squatting, hallucinations)

### End-users using AI system

1. Data Privacy
2. Data breaches
3. Aigorithm Bias & Fairness
4. Reliability & Robustness
5. Adversarial attacks
6. Transparency
7. Explainability
8. Governance, Regulation, and Compliance (GRC)
9. Secure integration & deployment
10. Vulnerability Management
11. Access Control
12. Vulnerabilities (go into one of the one of the previous sections: data poisoning, model poisoning, neural trojan attacks, model transfer attacks, malicious
    model files and dependencies, dependency confusion, typo-squatting, hallucinations)

Consider: 1. https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-218A.ipd.pdf
          2. https://docs.google.com/document/d/1P4J7LSaZYg6Sdg_uXy-bdf5FBgDxr7u5ytPe8YJl6-8/edit
          3. IEEE, ISO, EU Act conformance

Need to discuss about what 'AI' means, and what considerations for generative AI (separately or included in 'AI').




